Here is the code as plain text that you can copy:

Python

import streamlit as st
import requests
from bs4 import BeautifulSoup
from collections import Counter
import re
import pandas as pd
import time
import yfinance as yf
from datetime import datetime
import pytz
import concurrent.futures

# --- CONFIGURATION ---

# 1. CONSTANTS
PAGES_TO_SCRAPE = 50  # Fixed depth (approx 2,500 posts)
REFRESH_SECONDS = 300 # 5 Minutes
MAX_WORKERS = 10      # Speed boost: Scrapes 10 pages at once

# 2. MANUAL KOREAN MAP (Priority List)
MANUAL_MAP = {
    "BMNR": ["bmnr", "ë¹„íŠ¸ë§ˆì¸", "ë¹„ì— ì—”ì•Œ", "ì´ë”ë¦¬ì›€"],
    "RGTI": ["rgti", "ë¦¬ê²Œí‹°", "ì–‘ìž", "í€€í…€"],
    "NBIS": ["nbis", "ë„¤ë¹„ìš°ìŠ¤", "ì–€ë±ìŠ¤", "yandex"],
    "CRWV": ["crwv", "ì½”ì–´ìœ„ë¸Œ"],
    "OKLO": ["oklo", "ì˜¤í´ë¡œ", "ì•ŒíŠ¸ë§Œ", "ì›ì „"],
    "IREN": ["iren", "ì•„ì´ë¦¬ìŠ¤", "ì´ë Œ", "ì±„êµ´"],
    "SBET": ["sbet", "ìƒ¤í”„ë§í¬", "ì—ìŠ¤ë²³"],
    "SOXL": ["ì†ìŠ¬", "soxl", "í•„ë°˜ë„ì²´", "3ë°°", "ë°˜ë„ì²´3ë°°"],
    "SOXS": ["ì†ìŠ¤", "soxs", "ìˆìŠ¬", "ë°˜ë„ì²´ìˆ"],
    "TQQQ": ["í‹°í", "tqqq", "ë‚˜ìŠ¤ë‹¥3ë°°"],
    "SQQQ": ["ìŠ¤í", "sqqq", "ìˆí", "ë‚˜ìŠ¤ë‹¥ìˆ"],
    "SCHD": ["ìŠˆë“œ", "schd", "ë°°ë‹¹", "ì„±ìž¥ì£¼"],
    "JEPI": ["ì œí”¼", "jepi", "ì›”ë°°ë‹¹"],
    "TMF":  ["í‹°ì— ì—í”„", "tmf", "ì±„ê¶Œ3ë°°"],
    "TMV":  ["í‹°ì— ë¸Œì´", "tmv"],
    "BOIL": ["ë³´ì¼", "boil", "ê°€ìŠ¤"],
    "KOLD": ["ì½œë“œ", "kold", "ê°€ìŠ¤ìˆ"],
    "YINN": ["ì¸", "yinn", "ì¤‘êµ­3ë°°"],
    "YANG": ["ì–‘", "yang", "ì¤‘êµ­ìˆ"],
    "TSLA": ["í…ŒìŠ¬ë¼", "í…ŒìŠ¬í˜•", "tsla", "ë¨¸ìŠ¤í¬", "ì¼ë¡ ", "ì „ê¸°ì°¨", "ì²œìŠ¬ë¼"],
    "NVDA": ["ì—”ë¹„ë””ì•„", "ì—”ë¹„", "nvda", "í™©íšŒìž¥", "ì  ìŠ¨í™©", "ê°€ì£½ìžì¼“"],
    "AAPL": ["ì• í”Œ", "aapl", "ì‚¬ê³¼", "íŒ€ì¿¡"],
    "MSFT": ["ë§ˆì†Œ", "msft", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸"],
    "GOOGL": ["êµ¬ê¸€", "googl", "ì•ŒíŒŒë²³", "ê°“ê¸€"],
    "AMZN": ["ì•„ë§ˆì¡´", "amzn", "ë² ì¡°ìŠ¤"],
    "META": ["ë©”íƒ€", "meta", "íŽ˜ì´ìŠ¤ë¶", "ì£¼ì»¤ë²„ê·¸"],
    "NFLX": ["ë„·í”Œ", "nflx", "ë„·í”Œë¦­ìŠ¤"],
    "AMD":  ["ì•”ë“œ", "amd", "ë¦¬ì‚¬ìˆ˜"],
    "INTC": ["ì¸í…”", "intc"],
    "AVGO": ["ë¸Œë¡œë“œì»´", "avgo"],
    "TSM":  ["í‹°ì—ìŠ¤ì— ", "tsm", "ëŒ€ë§Œ"],
    "PLTR": ["íŒ”ëž€í‹°ì–´", "pltr", "íŒ”ëž€"],
    "SMCI": ["ìŠˆë§ˆì»´", "smci", "ìŠˆí¼ë§ˆì´í¬ë¡œ"],
    "MU":   ["ë§ˆì´í¬ë¡ ", "mu"],
    "IONQ": ["ì•„ì´ì˜¨í", "ì•„í", "ionq", "ê¹€ì •ìƒ"],
    "COIN": ["ì½”ì¸ë² ì´ìŠ¤", "coin", "ì½”ë² "],
    "MSTR": ["ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëž˜í‹°ì§€", "ë§ˆìŠ¤", "mstr", "ì„¸ì¼ëŸ¬"],
    "GME":  ["ê²Œìž„ìŠ¤íƒ‘", "gme", "ê²œìŠ¤"],
    "AMC":  ["ì—ì´ì— ì”¨", "amc"],
    "RKLB": ["ë¡œì¼“ëž©", "rklb", "ë¡œì¼“"],
    "ASTS": ["ì—ìŠ¤í‹°", "asts", "ìŠ¤íŽ˜ì´ìŠ¤ëª¨ë°”ì¼"],
    "JOBY": ["ì¡°ë¹„", "joby"],
    "LCID": ["ë£¨ì‹œë“œ", "lcid"],
    "RIVN": ["ë¦¬ë¹„ì•ˆ", "rivn"],
    "MULN": ["ë®¬ëŸ°", "muln"],
    "NKLA": ["ë‹ˆì½œë¼", "nkla"],
    "BYND": ["bynd", "ë¹„ìš˜ë“œë¯¸íŠ¸", "ë¹„ìš˜ë“œ", "ì½©ê³ ê¸°"],
    "CPNG": ["ì¿ íŒ¡", "cpng"],
    "O":    ["ë¦¬ì–¼í‹°ì¸ì»´", "ë¦¬ì–¼í‹°", "ì›”ë°°ë‹¹", "o"],
}

# 3. English words to ignore
IGNORE_WORDS = {
    'ETF', 'QQQ', 'AI', 'CEO', 'FOMC', 'CPI', 'PPI', 'GDP', 'VS', 'US', 'FED', 
    'SEC', 'IPO', 'PER', 'EPS', 'YOLO', 'LONG', 'SHORT', 'HOLD', 'BUY', 'SELL',
    'POV', 'USA', 'KRW', 'USD', 'NEWS', 'DCA', 'IMF', 'IRP', 'ISA', 'OTM', 'ITM',
    'GOD', 'RIP', 'WTF', 'OMG', 'BIG', 'PUT', 'CALL', 'MAX', 'MIN', 'ONE', 'TWO',
    'WOW', 'LOL', 'NEW', 'NOW', 'HOT', 'TOP', 'BEST', 'END', 'RUN', 'FLY', 'SEE',
    'WAY', 'YES', 'NO', 'AGAIN', 'TODAY', 'WEEK', 'MONTH', 'YEAR', 'TIME', 'LOVE',
    'ARE', 'CAN', 'CAT', 'EAT', 'BEAT', 'FUN', 'HAS', 'ALL', 'AGO', 'AWAY',
    'BET', 'BOX', 'CAR', 'CASH', 'DAY', 'DIG', 'DOG', 'DOOR', 'DRY', 'EYE',
    'FAT', 'FIT', 'FLY', 'FOX', 'GAS', 'GET', 'GO', 'GOLD', 'GOOD', 'GUY',
    'HE', 'HER', 'HEY', 'HIM', 'HIS', 'HOP', 'HOT', 'ICE', 'INK', 'JOB',
    'KEY', 'KIDS', 'LAW', 'LET', 'LOW', 'MAN', 'MAP', 'MET', 'MOM', 'NET',
    'OIL', 'OLD', 'OUT', 'OWN', 'PAY', 'PET', 'PLAY', 'RAW', 'RED', 'RUN',
    'SAD', 'SAFE', 'SAW', 'SAY', 'SEA', 'SEE', 'SET', 'SKY', 'SON', 'SUN',
    'TAX', 'TEA', 'TEN', 'THE', 'TIE', 'TOO', 'TOP', 'TRY', 'TWO', 'USE',
    'VAN', 'WAR', 'WAY', 'WE', 'WET', 'WIN', 'WOW', 'YES', 'YET', 'YOU', 'ZOO',
    'ART', 'ANT', 'BUG', 'BUS', 'CAP', 'CUT', 'DID', 'EGO', 'ERA', 'FAR',
    'FEW', 'FIX', 'FLU', 'FOG', 'GAP', 'GYM', 'HAT', 'HIT', 'HUG', 'HUT',
    'ILL', 'JAR', 'JET', 'JOY', 'KIT', 'LID', 'LIP', 'LOG', 'LOT', 'MAD',
    'MIX', 'MUD', 'MUG', 'NAP', 'NOD', 'NUT', 'OAK', 'ODD', 'OFF', 'PAN',
    'PEN', 'PIE', 'PIG', 'PIN', 'PIT', 'POD', 'POP', 'POT', 'PRO', 'RAG',
    'RAT', 'RIB', 'RID', 'RIG', 'RIM', 'RIP', 'ROD', 'ROT', 'RUB', 'RUG',
    'RUM', 'RUT', 'SAP', 'SIP', 'SIT', 'SIX', 'SKI', 'SOB', 'SOD', 'SOW',
    'SOY', 'SPA', 'SPY', 'SUB', 'SUM', 'TAB', 'TAG', 'TAN', 'TAP', 'TAR',
    'TIP', 'TOE', 'TON', 'TOW', 'TOY', 'TUB', 'TUG', 'URN', 'VET', 'VOW',
    'WAX', 'WEB', 'WED', 'WIG', 'WIT', 'WOE', 'WOK', 'WON', 'YAM', 'YAP',
    'YEA', 'YEN', 'YIP', 'ZIP', 'MEME', 'LIFE', 'LIVE', 'LOVE', 'HOPE', 'NEXT',
    'FAST', 'SAFE', 'BEST', 'REAL', 'TRUE', 'MAIN', 'POST', 'READ', 'LOOK',
    'HEAR', 'TELL', 'TALK', 'WALK', 'OPEN', 'SHUT', 'STOP', 'WAIT', 'STAY',
    'GROW', 'HELP', 'SEND', 'PICK', 'KEEP', 'HOLD', 'FIND', 'FALL', 'TURN',
    'MOVE', 'MEET', 'LEAD', 'LATE', 'HARD', 'EASY', 'COOL', 'COLD', 'WARM',
    'HIGH', 'DEEP', 'WIDE', 'LONG', 'FULL', 'FREE', 'RICH', 'POOR', 'NICE',
    'KIND', 'FAIR', 'FINE', 'BLUE', 'CME', 'CS', 'GS', 'MS', 'C', 'A', 'F', 'ONE'
}

# --- PAGE CONFIG ---
st.set_page_config(page_title="Korean Ant Sentiment Tracker", page_icon="ðŸœ", layout="wide")

# --- DATA LOADING ---
@st.cache_data(ttl=3600)
def load_sec_tickers():
    url = "https://www.sec.gov/files/company_tickers.json"
    headers = {"User-Agent": "StreamlitDashboard contact@example.com"}
    sec_map = {} 
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            data = response.json()
            suffixes = [" Inc.", " Corp.", " Corporation", " Ltd", " Co.", " PLC", " Group", " Holdings"]
            for entry in data.values():
                ticker = entry['ticker'].upper()
                raw_title = entry['title']
                clean = raw_title
                for s in suffixes:
                    clean = clean.replace(s, "").replace(s.lower(), "")
                clean = clean.strip().lower()
                sec_map[ticker] = [ticker.lower()]
                if len(clean) > 3:
                    sec_map[ticker].append(clean)
            return sec_map
        return {}
    except:
        return {}

@st.cache_data(ttl=300)
def get_price_changes(ticker_list):
    if not ticker_list: return {}
    tickers_str = " ".join(ticker_list)
    changes = {}
    try:
        data = yf.download(tickers_str, period="5d", progress=False)
        if 'Close' in data:
            closes = data['Close']
            def calc_change(series):
                if len(series) >= 2:
                    return ((series.iloc[-1] - series.iloc[-2]) / series.iloc[-2]) * 100
                return None
            if isinstance(closes, pd.DataFrame):
                for ticker in ticker_list:
                    try:
                        changes[ticker] = calc_change(closes[ticker].dropna())
                    except: changes[ticker] = None
            elif isinstance(closes, pd.Series):
                changes[ticker_list[0]] = calc_change(closes.dropna())
    except: pass
    return changes

# --- HELPER TO SCRAPE ONE PAGE ---
def scrape_single_page(gallery_id, page, headers):
    url = "https://gall.dcinside.com/mgallery/board/lists"
    params = {'id': gallery_id, 'page': page}
    titles = []
    try:
        r = requests.get(url, headers=headers, params=params, timeout=5)
        if r.status_code == 200:
            soup = BeautifulSoup(r.text, 'html.parser')
            for row in soup.select('.gall_list .ub-content'):
                t = row.select_one('.gall_tit a')
                if t: titles.append(re.sub(r'\[\d+\]$', '', t.text.strip()).strip())
    except: pass
    return titles

# --- SCRAPING FUNCTIONS (PARALLEL) ---
@st.cache_data(ttl=60)
def scrape_dc_gallery_parallel(gallery_id, pages=50):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36'}
    all_titles = []
    
    # Use ThreadPoolExecutor to fetch pages in parallel
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_page = {executor.submit(scrape_single_page, gallery_id, p, headers): p for p in range(1, pages + 1)}
        for future in concurrent.futures.as_completed(future_to_page):
            try:
                data = future.result()
                all_titles.extend(data)
            except Exception as exc:
                pass
                
    return all_titles

def process_sentiment(titles, sec_map):
    ticker_counter = Counter()
    
    full_map = sec_map.copy()
    for k, v in MANUAL_MAP.items():
        if k in full_map: full_map[k] = list(set(full_map[k] + v))
        else: full_map[k] = v

    for title in titles:
        title_lower = title.lower()
        found = set()
        
        for ticker, keywords in full_map.items():
            for k in keywords:
                if len(k) > 2 and k in title_lower: 
                    found.add(ticker)
                    break
        
        for cand in re.findall(r'\b[A-Z]{2,5}\b', title):
            if cand in full_map and cand not in IGNORE_WORDS:
                found.add(cand)
        
        ticker_counter.update(found)

    return ticker_counter, titles

# --- DASHBOARD LOGIC ---
st.title("ðŸœ Korean Ant Sentiment Tracker")

# Get Current Time (EST)
tz = pytz.timezone('US/Eastern')
now = datetime.now(tz).strftime("%Y-%m-%d %I:%M:%S %p %Z")
st.caption(f"Last Updated: **{now}** | Updates automatically every 5 minutes.")

# --- SIDEBAR (Minimal) ---
with st.sidebar:
    st.header("Settings")
    gallery = st.selectbox("Target Gallery", ["tenbagger", "stockus", "nasdaq", "bitcoins"], index=0)
    st.info(f"Depth: Fixed at {PAGES_TO_SCRAPE} pages")
    st.info("Mode: Automatic Refresh")

# --- MAIN EXECUTION ---
# No button check, just run immediately
with st.spinner(f"Scraping {PAGES_TO_SCRAPE} pages from DC Inside..."):
    # Load Data
    sec_map = load_sec_tickers()
    titles = scrape_dc_gallery_parallel(gallery, PAGES_TO_SCRAPE)

    if titles:
        # Process
        t_counts, raw = process_sentiment(titles, sec_map)
        
        # DataFrame
        df = pd.DataFrame.from_dict(t_counts, orient='index', columns=['Mentions']).sort_values('Mentions', ascending=False).head(20)
        
        # Prices
        if not df.empty:
            top_tickers = df.index.tolist()
            price_changes = get_price_changes(top_tickers)
            df['% Change'] = df.index.map(price_changes)

        # UI
        st.success(f"Analyzed {len(titles)} posts.")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("ðŸ† Leaderboard")
            st.bar_chart(df['Mentions'], color="#FF4B4B")
            
        with col2:
            st.subheader("ðŸ“Š Detailed Counts")
            if not df.empty:
                def color_change(val):
                    if pd.isna(val): return 'color: gray'
                    color = '#4CAF50' if val > 0 else '#FF4B4B' if val < 0 else 'gray'
                    return f'color: {color}'

                styled_df = df.style.map(color_change, subset=['% Change']).format({'% Change': lambda x: f'{x:+.2f}%' if pd.notnull(x) else 'N/A'})
                st.dataframe(styled_df, use_container_width=True)
            else:
                st.info("No tickers found.")

        with st.expander("Raw Titles"):
            st.write(raw)

    # AUTO REFRESH LOOP
    time.sleep(REFRESH_SECONDS)
    st.rerun()
