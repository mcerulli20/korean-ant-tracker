import streamlit as st
import requests
from bs4 import BeautifulSoup
from collections import Counter
import re
import pandas as pd
import time

# --- CONFIGURATION ---
# 1. YOUR CUSTOM TICKER LIST (Embedded directly here)
# Format: "TICKER": ["korean_slang", "lowercase", "other_nickname"]
TICKER_MAP = {
    # --- YOUR REQUESTED STOCKS ---
    "BMNR": ["bmnr", "ë¹„íŠ¸ë§ˆì¸", "ë¹„ì— ì—”ì•Œ", "ì´ë”ë¦¬ì›€"],
    "RGTI": ["rgti", "ë¦¬ê²Œí‹°", "ì–‘ì", "í€€í…€"],
    "NBIS": ["nbis", "ë„¤ë¹„ìš°ìŠ¤", "ì–€ë±ìŠ¤", "yandex"],
    "CRWV": ["crwv", "ì½”ì–´ìœ„ë¸Œ"],
    "OKLO": ["oklo", "ì˜¤í´ë¡œ", "ì•ŒíŠ¸ë§Œ", "ì›ì „"],

    # --- LEVERAGED ETFs (The Kings of DC Inside) ---
    "SOXL": ["ì†ìŠ¬", "soxl", "í•„ë°˜ë„ì²´", "3ë°°", "ë°˜ë„ì²´3ë°°"],
    "SOXS": ["ì†ìŠ¤", "soxs", "ìˆìŠ¬", "ë°˜ë„ì²´ìˆ"],
    "TQQQ": ["í‹°í", "tqqq", "ë‚˜ìŠ¤ë‹¥3ë°°"],
    "SQQQ": ["ìŠ¤í", "sqqq", "ìˆí", "ë‚˜ìŠ¤ë‹¥ìˆ"],
    "SCHD": ["ìŠˆë“œ", "schd", "ë°°ë‹¹"],
    "JEPI": ["ì œí”¼", "jepi"],

    # --- BIG TECH & POPULAR ---
    "TSLA": ["í…ŒìŠ¬ë¼", "í…ŒìŠ¬í˜•", "tsla", "ë¨¸ìŠ¤í¬", "ì¼ë¡ "],
    "NVDA": ["ì—”ë¹„ë””ì•„", "ì—”ë¹„", "nvda", "í™©íšŒì¥"],
    "AAPL": ["ì• í”Œ", "aapl", "ì‚¬ê³¼", "íŒ€ì¿¡"],
    "MSFT": ["ë§ˆì†Œ", "msft", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸"],
    "GOOGL": ["êµ¬ê¸€", "googl", "ì•ŒíŒŒë²³"],
    "AMZN": ["ì•„ë§ˆì¡´", "amzn"],
    "IONQ": ["ì•„ì´ì˜¨í", "ì•„í", "ionq"],
    "PLTR": ["íŒ”ë€í‹°ì–´", "pltr"],
    "COIN": ["ì½”ì¸ë² ì´ìŠ¤", "coin", "ì½”ë² "],
    "MSTR": ["ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€", "ë§ˆìŠ¤", "mstr"],
    "GME":  ["ê²Œì„ìŠ¤íƒ‘", "gme", "ê²œìŠ¤"],
}

# 2. English words to ignore in "Auto Discovery"
IGNORE_WORDS = {
    'ETF', 'QQQ', 'AI', 'CEO', 'FOMC', 'CPI', 'PPI', 'GDP', 'VS', 'US', 'FED', 
    'SEC', 'IPO', 'PER', 'EPS', 'YOLO', 'LONG', 'SHORT', 'HOLD', 'BUY', 'SELL',
    'POV', 'USA', 'KRW', 'USD', 'NEWS', 'DCA', 'IMF', 'IRP', 'ISA', 'OTM', 'ITM',
    'GOD', 'RIP', 'WTF', 'OMG', 'BIG', 'PUT', 'CALL', 'MAX', 'MIN', 'ONE', 'TWO',
    'WOW', 'LOL', 'NEW', 'NOW', 'HOT', 'TOP', 'BEST', 'END', 'RUN', 'FLY', 'SEE',
    'WAY', 'YES', 'NO', 'AGAIN', 'TODAY', 'WEEK', 'MONTH', 'YEAR', 'TIME', 'LOVE'
}

# 3. Korean words to ignore in "Mystery Trend Spotter"
KOREAN_STOPWORDS = {
    'ì˜¤ëŠ˜', 'ì§€ê¸ˆ', 'ì§„ì§œ', 'ì´ê±°', 'ê·¼ë°', 'í•˜ëŠ”', 'ë‚´ê°€', 'ì¡´ë‚˜', 'ì‹œë°œ', 'ã…‹ã…‹', 'ã…ã…',
    'ã… ã… ', 'ë§¤ìˆ˜', 'ë§¤ë„', 'ì‚¬ëŒ', 'ìƒê°', 'ë¯¸ì¥', 'êµ­ì¥', 'ì£¼ì‹', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì¢€',
    'ë‹¤ì‹œ', 'ë³´ë©´', 'ê°€ì¦ˆì•„', 'ì˜¤ëŠ˜ì˜', 'ë¯¸êµ­', 'ë‹¬ëŸ¬', 'ì½”ì¸', 'ë¹„íŠ¸', 'ë‚˜ìŠ¤ë‹¥', 'ìˆëŠ”',
    'í•˜ê³ ', 'ì•„ë‹ˆ', 'ê·¸ëƒ¥', 'ë§ì´', 'ë„ˆë¬´', 'ê°œë¯¸', 'í˜•ë“¤', 'ê°ˆê¹Œ', 'ë§ê¹Œ', 'ì–¸ì œ', 'ì—­ì‹œ',
    'ì´ì œ', 'ì´ë ‡ê²Œ', 'ì§€ìˆ˜', 'í•˜ë½', 'ìƒìŠ¹', 'ë³¸ì¥', 'í”„ë¦¬', 'ê±°ë˜', 'ìˆ˜ìµ', 'ì†ì‹¤', 'ì œë°œ',
    'ë‚˜ëŠ”', 'ì˜¤ë¥¼', 'ë‚´ë¦´', 'ë¡±ì¶©', 'ìˆì¶©', 'ê°™ë‹¤', 'ê°™ì€', 'í•´ì„œ', 'í•˜ë©´', 'ì˜¤ë¥´', 'ë‚´ë¦¬'
}

# --- PAGE CONFIG ---
st.set_page_config(
    page_title="Korean Ant Sentiment Tracker",
    page_icon="ğŸœ",
    layout="wide"
)

# --- SCRAPING FUNCTIONS ---
@st.cache_data(ttl=60)
def scrape_dc_gallery(gallery_id, pages=10, mode="all"):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    }
    base_url = "https://gall.dcinside.com/mgallery/board/lists"
    all_titles = []
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    
    for page in range(1, pages + 1):
        progress_text.text(f"ğŸœ Collecting Page {page}/{pages}...")
        params = {'id': gallery_id, 'page': page}
        if mode == "recommend":
            params['exception_mode'] = 'recommend'

        try:
            response = requests.get(base_url, headers=headers, params=params, timeout=5)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                rows = soup.select('.gall_list .ub-content')
                for row in rows:
                    title_element = row.select_one('.gall_tit a')
                    if title_element:
                        full_text = title_element.text.strip()
                        # Clean title: Remove [Reply Count] like [32]
                        clean_title = re.sub(r'\[\d+\]$', '', full_text).strip()
                        if clean_title:
                            all_titles.append(clean_title)
            time.sleep(0.15) 
        except Exception as e:
            st.error(f"Error on page {page}: {e}")
        
        progress_bar.progress(page / pages)
    
    progress_text.empty()
    progress_bar.empty()
    return all_titles

def process_sentiment(titles):
    ticker_counter = Counter()
    word_counter = Counter()
    
    # Create a set of all known keywords (tickers + slang) to exclude them from the "Unknown" list
    all_known_keywords = set()
    for keywords in TICKER_MAP.values():
        for k in keywords:
            all_known_keywords.add(k)

    for title in titles:
        title_lower = title.lower()
        found_in_title = set()
        
        # 1. Known Ticker Check (Map)
        for ticker, keywords in TICKER_MAP.items():
            for keyword in keywords:
                if keyword in title_lower:
                    found_in_title.add(ticker)
                    break 
        
        # 2. Auto Discovery (English Uppercase Words)
        # Regex finds 2-5 letter uppercase words (e.g. RGTI, TSLA)
        candidates = re.findall(r'\b[A-Z]{2,5}\b', title)
        for cand in candidates:
            if cand not in IGNORE_WORDS and cand not in found_in_title:
                # If we found it via auto-discovery, treat it as a hit
                # But check if we already mapped it to a key to avoid duplicates
                if cand in TICKER_MAP:
                    found_in_title.add(cand)
                else:
                    found_in_title.add(cand)

        ticker_counter.update(found_in_title)

        # 3. Mystery Word Spotter (Korean Only)
        # This finds high-frequency Korean words that are NOT in your ticker list yet.
        words = title_lower.split()
        for word in words:
            # Remove punctuation
            word = re.sub(r'[^\w\s]', '', word)
            
            # Logic: Must be Korean AND not a known ticker keyword AND not a stopword
            if re.search(r'[ê°€-í£]+', word): 
                if word not in all_known_keywords and word not in KOREAN_STOPWORDS:
                    word_counter[word] += 1

    return ticker_counter, word_counter, titles

# --- DASHBOARD UI ---
st.title("ğŸœ Korean Ant Sentiment Tracker")
st.markdown("""
Tracking real-time mentions on **DC Inside (Mijugal)**.  
Includes **Auto-Discovery** for unknown tickers + **Mystery Word** spotting.
""")

with st.sidebar:
    st.header("âš™ï¸ Scanner Settings")
    gallery_id = st.selectbox("Target Gallery", ["tenbagger", "stockus", "nasdaq", "bitcoins"], index=0)
    pages = st.slider("Depth (Pages)", 1, 100, 10, help="50 pages â‰ˆ 2,500 posts.")
    mode = st.radio("Filter Mode", ["all", "recommend"], index=0, format_func=lambda x: "ğŸ”¥ New Posts (High Vol)" if x == "all" else "ğŸ’ Concept (Best Of)")
    st.divider()
    if st.button("ğŸš€ START SCAN", type="primary", use_container_width=True):
        st.session_state['run'] = True

# --- RESULTS ---
if st.session_state.get('run'):
    with st.spinner("Scraping DC Inside..."):
        titles = scrape_dc_gallery(gallery_id, pages, mode)
    
    if titles:
        ticker_counts, word_counts, raw_titles = process_sentiment(titles)
        
        # Create DataFrames
        df_tickers = pd.DataFrame.from_dict(ticker_counts, orient='index', columns=['Mentions']).sort_values(by='Mentions', ascending=False).head(20)
        df_words = pd.DataFrame.from_dict(word_counts, orient='index', columns=['Count']).sort_values(by='Count', ascending=False).head(20)
        
        # Display Results
        st.success(f"Successfully analyzed {len(titles)} posts!")
        
        col1, col2 = st.columns([1.5, 1])
        
        with col1:
            st.subheader("ğŸ† Top Identified Tickers")
            if not df_tickers.empty:
                st.bar_chart(df_tickers, color="#FF4B4B")
            else:
                st.info("No known tickers found. Try increasing depth.")

        with col2:
            st.subheader("â“ Mystery Trend Spotter")
            st.caption("Top Korean words NOT in your ticker map. If you see a stock name here, add it to the code!")
            st.dataframe(df_words, use_container_width=True)

        with st.expander("ğŸ” Inspect Raw Post Titles"):
            st.write(raw_titles)
    else:
        st.error("No data retrieved.")